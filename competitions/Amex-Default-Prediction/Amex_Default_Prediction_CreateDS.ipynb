{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amex-Default-Prediction-CreateDS.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOGsS5/uyVaFOd66lSeaUY4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing for colab"
      ],
      "metadata": {
        "id": "whFMYLB1u7rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "MwbPTJbLu1oU",
        "outputId": "ef9abce8-33ff-4675-eda9-c5da229c1b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && cp /content/drive/MyDrive/backups/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "o1piHn7q7SRh",
        "outputId": "680139d8-2cda-48fb-aca0-07ab5caaa164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c amex-default-prediction -f sample_submission.csv\n",
        "!unzip /content/sample_submission.csv.zip"
      ],
      "metadata": {
        "id": "jeIvlVN0vQ08",
        "outputId": "cd2dd1e0-3fdf-4eba-b5aa-a756119d5c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/32.4M [00:00<?, ?B/s]\r 49% 16.0M/32.4M [00:00<00:00, 167MB/s]\n",
            "\r100% 32.4M/32.4M [00:00<00:00, 211MB/s]\n",
            "Archive:  /content/sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c amex-default-prediction -f train_labels.csv\n",
        "!unzip /content/train_labels.csv.zip"
      ],
      "metadata": {
        "id": "PfGNoPMDxoPk",
        "outputId": "efdca2ca-6041-4308-a90e-a0f412ede6bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train_labels.csv.zip to /content\n",
            "\r  0% 0.00/16.2M [00:00<?, ?B/s]\r 80% 13.0M/16.2M [00:00<00:00, 135MB/s]\n",
            "\r100% 16.2M/16.2M [00:00<00:00, 149MB/s]\n",
            "Archive:  /content/train_labels.csv.zip\n",
            "  inflating: train_labels.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d raddar/amex-data-integer-dtypes-parquet-format\n",
        "!unzip /content/amex-data-integer-dtypes-parquet-format.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdxHrT8y0iJY",
        "outputId": "e5dddd31-2302-4026-f10c-4623d7c8230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading amex-data-integer-dtypes-parquet-format.zip to /content\n",
            "100% 4.06G/4.07G [00:16<00:00, 197MB/s]\n",
            "100% 4.07G/4.07G [00:16<00:00, 265MB/s]\n",
            "Archive:  /content/amex-data-integer-dtypes-parquet-format.zip\n",
            "  inflating: test.parquet            "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "34iwBmT90mYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import gc; gc.enable()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "import itertools\n",
        "\n",
        "def get_difference(data, num_features):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
        "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    return df1\n",
        "\n",
        "# ====================================================\n",
        "# Read & preprocess data and save it to disk\n",
        "# ====================================================\n",
        "def read_preprocess_data():\n",
        "    train = pd.read_parquet('/content/train.parquet')\n",
        "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
        "\n",
        "    # use_features = [\n",
        "    #     'B_18', 'B_2', 'B_3', 'B_33', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_61', 'P_2', # the most important\n",
        "    #     'B_15', 'B_27', 'D_106', 'D_109', 'D_116', 'D_144', 'D_69', 'D_73', 'R_18', 'R_23', 'R_28', 'S_12', 'S_18', 'S_19'\n",
        "    # ]\n",
        "\n",
        "    not_use_features = [\n",
        "        # 'B_1', 'B_11', 'B_23', 'B_37', 'D_103', 'D_118', 'D_137', 'D_139', 'D_141', 'D_74', 'D_77', 'S_24',\n",
        "    ]\n",
        "\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\",\n",
        "    ]\n",
        "    bin_features = ['R_1', 'R_2', 'R_4', 'R_15', 'R_19', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_28', 'R_7', 'R_12', 'R_14',\n",
        "                    'B_8', 'B_19', 'B_20', 'B_32', 'B_33',\n",
        "                    'P_4', 'S_6',\n",
        "                    'D_86', 'D_92', 'D_93', 'D_94', 'D_96', 'D_103', 'D_104', 'D_108', 'D_109', 'D_112', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131',\n",
        "                    'D_133', 'D_135','D_137', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', 'D_145'\n",
        "                    ]\n",
        "\n",
        "    num_features = [col for col in features if col not in cat_features]\n",
        "    num_features = [col for col in num_features if col not in not_use_features]\n",
        "\n",
        "    cat_features = [col for col in cat_features if col not in not_use_features]\n",
        "\n",
        "    for feature in bin_features:\n",
        "        train.loc[train[feature] != 0, feature] = 1\n",
        "\n",
        "    # train.S_2 = pd.to_datetime(train.S_2)\n",
        "    # train_s2_agg = train.groupby(\"customer_ID\")['S_2'].agg(['first', 'last'])\n",
        "    # train_s2_agg = (train_s2_agg['last'] - train_s2_agg['first']).dt.days\n",
        "    # train_s2_agg = train_s2_agg.to_frame()\n",
        "    # train_s2_agg.columns = [ 'S_2_lag']\n",
        "    # train_s2_agg.reset_index(inplace=True)\n",
        "    \n",
        "    # Train FE\n",
        "    print('Starting train feature extraction')\n",
        "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
        "    train_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # for col in train_num_agg.columns[1:]:\n",
        "    #     train_num_agg[col] = train_num_agg[col] / train_s2_agg.S_2_lag\n",
        "\n",
        "    # Lag Features\n",
        "    for col in train_num_agg:\n",
        "        # for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "        for col_2 in ['first', 'mean']:\n",
        "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
        "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
        "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
        "\n",
        "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
        "    train_cat_agg.reset_index(inplace = True)\n",
        "    \n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
        "\n",
        "    train_labels = pd.read_csv('/content/train_labels.csv')\n",
        "    \n",
        "    # Get the difference\n",
        "    # train_diff = get_difference(train, num_features)\n",
        "    # train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "\n",
        "    print('Train shape: ', train.shape)\n",
        "    del train_num_agg, train_cat_agg\n",
        "    gc.collect() \n",
        "\n",
        "    train.to_parquet('train_fe_plus_plus.parquet')\n",
        "    del train\n",
        "    gc.collect()\n",
        "    \n",
        "    # Test FE\n",
        "    test = pd.read_parquet('/content/test.parquet')\n",
        "    print('Starting test feature extraction')\n",
        "\n",
        "    for feature in bin_features:\n",
        "        test.loc[test[feature] != 0, feature] = 1\n",
        "\n",
        "    # test.S_2 = pd.to_datetime(test.S_2)\n",
        "    # test_s2_agg = test.groupby(\"customer_ID\")['S_2'].agg(['first', 'last'])\n",
        "    # test_s2_agg = (test_s2_agg['last'] - test_s2_agg['first']).dt.days\n",
        "    # test_s2_agg = test_s2_agg.to_frame()\n",
        "    # test_s2_agg.columns = [ 'S_2_lag']\n",
        "    # test_s2_agg.reset_index(inplace=True)\n",
        "\n",
        "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "    test_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # for col in test_num_agg.columns[1:]:\n",
        "    #     test_num_agg[col] = test_num_agg[col] / test_s2_agg.S_2_lag\n",
        "\n",
        "    # Lag Features\n",
        "    for col in test_num_agg:\n",
        "        # for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "        for col_2 in ['first', 'mean']:\n",
        "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
        "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
        "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
        "    \n",
        "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "    test_cat_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
        "\n",
        "    # Get the difference\n",
        "    # test_diff = get_difference(test, num_features)\n",
        "    # test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
        "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID')\n",
        "\n",
        "    print('Test shape: ', test.shape)\n",
        "    del test_num_agg, test_cat_agg\n",
        "    gc.collect()\n",
        "    \n",
        "    \n",
        "    # Save files to disk\n",
        "    test.to_parquet('test_fe_plus_plus.parquet')\n",
        "    del test\n",
        "    gc.collect()\n",
        "    \n",
        "# Read & Preprocess Data\n",
        "read_preprocess_data()"
      ],
      "metadata": {
        "id": "RUcDu4Y00is4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir amex-fe-plus2"
      ],
      "metadata": {
        "id": "6NBwlG5F158O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/train_fe_plus_plus.parquet /content/amex-fe-plus2/\n",
        "!mv /content/test_fe_plus_plus.parquet /content/amex-fe-plus2/"
      ],
      "metadata": {
        "id": "7hS5QEfe3nL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets init -p /content/amex-fe-plus2"
      ],
      "metadata": {
        "id": "6IfjfgG657Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/amex-fe-plus2/dataset-metadata.json\n",
        "{\n",
        "  \"title\": \"Amex-FE-Plus2\",\n",
        "  \"id\": \"ryuina/amex-fe-plus2\",\n",
        "  \"licenses\": [\n",
        "    {\n",
        "      \"name\": \"CC0-1.0\"\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "jCz5AoIe55_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets version -p /content/amex-fe-plus2 -m \"Convert binary features\"\n",
        "\n",
        "# for the first time\n",
        "# !kaggle datasets create -p /content/amex-fe-plus2"
      ],
      "metadata": {
        "id": "dvPRC0IB4E7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fb7bcc-a908-48ef-c237-b6f148d4a4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting upload for file train_fe_plus_plus.parquet\n",
            "100% 1.53G/1.53G [00:18<00:00, 87.0MB/s]\n",
            "Upload successful: train_fe_plus_plus.parquet (2GB)\n",
            "Starting upload for file test_fe_plus_plus.parquet\n",
            "100% 2.75G/2.75G [00:34<00:00, 85.3MB/s]\n",
            "Upload successful: test_fe_plus_plus.parquet (3GB)\n",
            "Dataset version is being created. Please check progress at https://www.kaggle.com/ryuina/amex-fe-plus2\n"
          ]
        }
      ]
    }
  ]
}